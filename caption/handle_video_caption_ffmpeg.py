import os
import subprocess
import json
from pathlib import Path
from PIL import Image
import torch
from unittest.mock import patch
from transformers.dynamic_module_utils import get_imports
from transformers import AutoModelForCausalLM, AutoProcessor
from openai import OpenAI

# åˆå§‹åŒ–åƒé—®å®¢æˆ·ç«¯
def init_qwen_client():
    """åˆå§‹åŒ–åƒé—®å¤§æ¨¡å‹å®¢æˆ·ç«¯"""
    api_key = "sk-cebe1cdb99ed44a69d41f194c25ece92"
    if not api_key:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ï¼Œåƒé—®ä¼˜åŒ–åŠŸèƒ½å°†è¢«è·³è¿‡")
        return None

    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        return client
    except Exception as e:
        print(f"âŒ åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def optimize_prompt_with_qwen(caption, client):
    """
    ä½¿ç”¨åƒé—®å¤§æ¨¡å‹ä¼˜åŒ–æç¤ºè¯ï¼Œè¾“å‡ºé€‚åˆè§†é¢‘ç”Ÿæˆçš„è‹±æ–‡æç¤ºè¯

    Args:
        caption: åŸå§‹ä¸­æ–‡æç¤ºè¯
        client: åƒé—®å®¢æˆ·ç«¯

    Returns:
        str: ä¼˜åŒ–åçš„è‹±æ–‡æç¤ºè¯
    """
    if not client:
        return caption

    if not caption or caption.strip() == "":
        return caption

    try:
        prompt = f"""è¯·å°†ä»¥ä¸‹å›¾åƒæç¤ºè¯ä¼˜åŒ–ä¸ºè‹±æ–‡è§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼Œè¦æ±‚ï¼š

1. ä¿æŒæ ¸å¿ƒå†…å®¹ä¸å˜
2. å°†æ¶‰åŠimageç­‰å›¾åƒç›¸å…³å­—çœ¼çš„æè¿°éƒ½å»æ‰
3. ä¿æŒç®€æ´æ˜äº†ï¼Œé•¿åº¦åœ¨50-100è¯ä¹‹é—´
5. ç›´æ¥è¾“å‡ºè‹±æ–‡æç¤ºè¯ï¼Œä¸éœ€è¦ä»»ä½•è§£é‡Š

å›¾åƒæç¤ºè¯ï¼š
{caption}

è¯·è¾“å‡ºä¼˜åŒ–åçš„è‹±æ–‡æç¤ºè¯ï¼š"""

        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {'role': 'user', 'content': prompt}
            ]
        )

        optimized_caption = completion.choices[0].message.content.strip()

        # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤å¯èƒ½çš„å¼•å·æˆ–å‰ç¼€ï¼‰
        if optimized_caption.startswith('"') and optimized_caption.endswith('"'):
            optimized_caption = optimized_caption[1:-1]
        if optimized_caption.startswith("æç¤ºè¯ï¼š"):
            optimized_caption = optimized_caption[4:]

        print(f"  - âœ… åƒé—®ä¼˜åŒ–å®Œæˆ")
        print(f"  - åŸå§‹: {caption[:50]}...")
        print(f"  - ä¼˜åŒ–å: {optimized_caption[:50]}...")

        return optimized_caption

    except Exception as e:
        print(f"  - âŒ åƒé—®ä¼˜åŒ–å¤±è´¥: {e}")
        return caption

def fixed_get_imports(filename):
    """ä¿®å¤transformersåŠ¨æ€æ¨¡å—å¯¼å…¥é—®é¢˜"""
    if not str(filename).endswith("modeling_florence2.py"):
        return get_imports(filename)
    imports = get_imports(filename)
    try:
        imports.remove("flash_attn")
    except:
        print(f"No flash_attn import to remove")
        pass
    return imports

def generate_caption_for_image(image, caption_method="extra_mixed", model_name="promptgen_base_v2.0",
                              max_new_tokens=1024, num_beams=4, random_prompt=False):
    """
    å¯¹å•ä¸ªå›¾åƒè¿›è¡Œæç¤ºè¯åæ¨çš„æ ¸å¿ƒå‡½æ•°

    Args:
        image: PIL Imageå¯¹è±¡
        caption_method: æç¤ºè¯ç”Ÿæˆæ–¹æ³• ('tags', 'simple', 'detailed', 'extra', 'mixed', 'extra_mixed', 'analyze')
        model_name: æ¨¡å‹åç§°
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        num_beams: beam searchæ•°é‡
        random_prompt: æ˜¯å¦éšæœºç”Ÿæˆ

    Returns:
        str: ç”Ÿæˆçš„æç¤ºè¯
    """
    # è®¾ç½®è®¾å¤‡ç²¾åº¦
    attention = 'sdpa'
    precision = 'fp16'
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    dtype = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}[precision]

    # é€‰æ‹©æ¨¡å‹
    hg_model = 'MiaoshouAI/Florence-2-base-PromptGen-v2.0'
    if model_name == 'promptgen_large_v2.0':
        hg_model = 'MiaoshouAI/Florence-2-large-PromptGen-v2.0'

    model_name_short = hg_model.rsplit('/', 1)[-1]
    model_path = f"/mnt/disk0/pretrained_models/{model_name_short}"

    # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ‰“å°ä¿¡æ¯ï¼ˆå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦ä¸‹è½½ï¼‰
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print(f"è¯·ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹åˆ°: {model_path}")
        print(f"æˆ–ä¿®æ”¹model_pathä¸ºæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return ""

    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    try:
        with patch("transformers.dynamic_module_utils.get_imports", fixed_get_imports):
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                attn_implementation=attention,
                device_map=device,
                torch_dtype=dtype,
                trust_remote_code=True
            ).to(device)

        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return ""

    # æ ¹æ®caption_methodè®¾ç½®æç¤ºè¯
    if caption_method == 'tags':
        prompt = "<GENERATE_TAGS>"
    elif caption_method == 'simple':
        prompt = "<CAPTION>"
    elif caption_method == 'detailed':
        prompt = "<DETAILED_CAPTION>"
    elif caption_method == 'extra':
        prompt = "<MORE_DETAILED_CAPTION>"
    elif caption_method == 'mixed':
        prompt = "<MIX_CAPTION>"
    elif caption_method == 'extra_mixed':
        prompt = "<MIX_CAPTION_PLUS>"
    else:
        prompt = "<ANALYZE>"

    # å¤„ç†å›¾åƒå¹¶ç”Ÿæˆæç¤ºè¯
    try:
        inputs = processor(text=prompt, images=image, return_tensors="pt", do_rescale=False).to(dtype).to(device)

        do_sample = True if random_prompt else False

        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=max_new_tokens,
            early_stopping=False,
            do_sample=do_sample,
            num_beams=num_beams,
        )

        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=prompt,
            image_size=(image.width, image.height)
        )

        return parsed_answer[prompt]
    except Exception as e:
        print(f"ç”Ÿæˆæç¤ºè¯å¤±è´¥: {e}")
        return ""

def process_video_directory(video_dir, output_dir, prompt_prefix="", caption_method="extra_mixed", use_qwen_optimize=True, frame_number=0):
    """
    å¤„ç†è§†é¢‘ç›®å½•çš„ä¸»å‡½æ•°

    Args:
        video_dir: è¾“å…¥è§†é¢‘ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        prompt_prefix: æç¤ºè¯å‰ç¼€
        caption_method: æç¤ºè¯ç”Ÿæˆæ–¹æ³•
        use_qwen_optimize: æ˜¯å¦ä½¿ç”¨åƒé—®ä¼˜åŒ–æç¤ºè¯
        frame_number: ç”¨äºæç¤ºè¯åæ¨çš„å¸§å·ï¼ˆä»0å¼€å§‹ï¼‰
    """
    # åˆå§‹åŒ–åƒé—®å®¢æˆ·ç«¯
    qwen_client = None
    if use_qwen_optimize:
        print("ğŸ”„ åˆå§‹åŒ–åƒé—®å®¢æˆ·ç«¯...")
        qwen_client = init_qwen_client()
        if qwen_client:
            print("âœ… åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âš ï¸  åƒé—®ä¼˜åŒ–åŠŸèƒ½å°†è¢«ç¦ç”¨")
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # è·å–æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v']
    video_files = []

    for file in os.listdir(video_dir):
        file_ext = os.path.splitext(file)[1].lower()
        if file_ext in video_extensions:
            video_files.append(os.path.join(video_dir, file))

    if not video_files:
        print(f"åœ¨ç›®å½• {video_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return

    # æŒ‰æ–‡ä»¶åæ’åº
    video_files.sort()

    print(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

    # å¤„ç†æ¯ä¸ªè§†é¢‘
    for i, video_path in enumerate(video_files, 1):
        video_name = os.path.splitext(os.path.basename(video_path))[0]

        print(f"\nå¤„ç†è§†é¢‘ {i}/{len(video_files)}: {video_name}")

        # 1. è½¬æ¢è§†é¢‘æ ¼å¼ä¸º16fpsçš„mp4
        output_video_path = os.path.join(output_dir, f"{i}.mp4")
        first_frame_path = os.path.join(output_dir, f"{i}.jpg")

        print(f"  - ä½¿ç”¨ffmpegè½¬æ¢è§†é¢‘ä¸º16fpså¹¶æå–ç¬¬{frame_number}å¸§...")

        # ä½¿ç”¨ffmpegå‘½ä»¤ï¼š
        # -r 16: è®¾ç½®å¸§ç‡ä¸º16fps
        # -y: è‡ªåŠ¨è¦†ç›–åŒåæ–‡ä»¶
        # -i: è¾“å…¥æ–‡ä»¶
        # -vf "select=eq(n\,0)": åªé€‰æ‹©ç¬¬0å¸§ï¼ˆé¦–å¸§ï¼‰
        # -vframes 1: åªæå–ä¸€å¸§
        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-r", "16",  # è®¾ç½®å¸§ç‡ä¸º16fps
            "-y",  # è‡ªåŠ¨è¦†ç›–åŒåæ–‡ä»¶
            output_video_path
        ]

        try:
            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
            print(f"  - âœ… è§†é¢‘è½¬æ¢å®Œæˆ: {os.path.basename(output_video_path)}")
        except subprocess.CalledProcessError:
            print(f"  - âŒ è§†é¢‘è½¬æ¢å¤±è´¥: {video_name}")
            continue
        except FileNotFoundError:
            print("  - âŒ é”™è¯¯: æœªæ‰¾åˆ°ffmpegï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶é…ç½®ç¯å¢ƒå˜é‡")
            break

        # æå–æŒ‡å®šå¸§
        extract_frame_cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", f"select=eq(n\\,{frame_number})",
            "-vframes", "1",
            "-y",
            first_frame_path
        ]

        try:
            subprocess.run(extract_frame_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
            print(f"  - âœ… ç¬¬{frame_number}å¸§æå–å®Œæˆ: {os.path.basename(first_frame_path)}")
        except subprocess.CalledProcessError:
            print(f"  - âŒ ç¬¬{frame_number}å¸§æå–å¤±è´¥: {video_name}")
            continue
        except FileNotFoundError:
            print("  - âŒ é”™è¯¯: æœªæ‰¾åˆ°ffmpegï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶é…ç½®ç¯å¢ƒå˜é‡")
            break

        # 2. å¯¹æŒ‡å®šå¸§è¿›è¡Œæç¤ºè¯åæ¨
        if os.path.exists(first_frame_path):
            print(f"  - å¯¹ç¬¬{frame_number}å¸§è¿›è¡Œæç¤ºè¯åæ¨...")

            # è¯»å–å›¾åƒ
            image = Image.open(first_frame_path).convert("RGB")

            # ç”Ÿæˆæç¤ºè¯
            caption = generate_caption_for_image(
                image,
                caption_method=caption_method,
                max_new_tokens=1024,
                num_beams=4,
                random_prompt=False
            )

            if caption:
                # 3. ä½¿ç”¨åƒé—®ä¼˜åŒ–æç¤ºè¯
                if qwen_client:
                    print(f"  - ä½¿ç”¨åƒé—®ä¼˜åŒ–æç¤ºè¯...")
                    optimized_caption = optimize_prompt_with_qwen(caption, qwen_client)
                else:
                    optimized_caption = caption

                # 4. æ·»åŠ å‰ç¼€å¹¶ä¿å­˜ä¸ºtxtæ–‡ä»¶
                final_caption = f"{prompt_prefix} {optimized_caption}" if prompt_prefix else optimized_caption
                txt_path = os.path.join(output_dir, f"{i}.txt")

                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(final_caption.strip())

                print(f"  - ä¿å­˜æç¤ºè¯åˆ°: {os.path.basename(txt_path)}")
                print(f"  - æç¤ºè¯é¢„è§ˆ: {final_caption[:100]}...")
            else:
                print(f"  - æç¤ºè¯ç”Ÿæˆå¤±è´¥")
        else:
            print(f"  - è­¦å‘Š: ç¬¬{frame_number}å¸§æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æç¤ºè¯ç”Ÿæˆ")

    # åˆ é™¤æ‰€æœ‰ç”Ÿæˆçš„å¸§å›¾ç‰‡
    print(f"\nğŸ—‘ï¸  æ¸…ç†å¸§å›¾ç‰‡...")
    for i in range(1, len(video_files) + 1):
        first_frame_path = os.path.join(output_dir, f"{i}.jpg")
        if os.path.exists(first_frame_path):
            try:
                os.remove(first_frame_path)
                print(f"  - âœ… å·²åˆ é™¤: {os.path.basename(first_frame_path)}")
            except Exception as e:
                print(f"  - âŒ åˆ é™¤å¤±è´¥ {os.path.basename(first_frame_path)}: {e}")

    print(f"âœ… å¸§å›¾ç‰‡æ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–é…ç½®æ–‡ä»¶
    config_path = os.path.join(os.path.dirname(__file__), "config.json")

    if not os.path.exists(config_path):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        video_directory = config.get("video_directory", "")
        output_directory = config.get("output_directory", "")
        prompt_prefix = config.get("prompt_prefix", "")
        caption_method = config.get("caption_method", "detailed")
        use_qwen_optimize = config.get("use_qwen_optimize", True)
        frame_number = config.get("frame_number", 0)

        if not video_directory:
            print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªè®¾ç½® video_directory")
            return

        if not output_directory:
            print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªè®¾ç½® output_directory")
            return

    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {e}")
        return
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return

    if not os.path.exists(video_directory):
        print(f"é”™è¯¯: è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {video_directory}")
        return

    print(f"\nå¼€å§‹å¤„ç†è§†é¢‘...")
    print(f"è§†é¢‘ç›®å½•: {video_directory}")
    print(f"è¾“å‡ºç›®å½•: {output_directory}")
    print(f"æç¤ºè¯å‰ç¼€: {prompt_prefix if prompt_prefix else 'æ— '}")
    print(f"æç¤ºè¯æ–¹æ³•: {caption_method}")
    print(f"åƒé—®ä¼˜åŒ–: {'å¯ç”¨' if use_qwen_optimize else 'ç¦ç”¨'}")
    print(f"ä½¿ç”¨å¸§å·: {frame_number}")
    print("-" * 50)

    try:
        process_video_directory(video_directory, output_directory, prompt_prefix, caption_method, use_qwen_optimize, frame_number)
        print("\nå¤„ç†å®Œæˆ!")
    except Exception as e:
        print(f"\nå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# /mnt/disk0/loraæ•°æ®é›†/2
# /mnt/disk0/train_data/2
# A trending dance move, arms and legs rapidly crisscrossing in a fast-paced dance.
# The character jumps up, turns body sideways to the camera, bends over, and starts twerking.
# detailed
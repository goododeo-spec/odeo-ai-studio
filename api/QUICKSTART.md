# Diffusion-Pipe API å¿«é€Ÿå…¥é—¨æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£å°†å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ Diffusion-Pipe çš„ Flask API æœåŠ¡ï¼Œå®ç° GPU ç®¡ç†ã€è®­ç»ƒä»»åŠ¡è°ƒåº¦ç­‰åŠŸèƒ½ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd /root/diffusion-pipe/api
make install
```

### 2. å¯åŠ¨ API æœåŠ¡

```bash
# å¼€å‘æ¨¡å¼ (è°ƒè¯•å¼€å¯)
make dev

# æˆ–åå°è¿è¡Œ
make background
```

### 3. æµ‹è¯• API

```bash
# è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
make test

# æˆ–è¿è¡Œæ¼”ç¤ºè„šæœ¬
python demo.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
api/
â”œâ”€â”€ app.py              # Flask åº”ç”¨ä¸»æ–‡ä»¶
â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”œâ”€â”€ run.py              # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ Makefile            # ä¾¿æ·å‘½ä»¤
â”œâ”€â”€ requirements.txt    # ä¾èµ–åŒ…
â”‚
â”œâ”€â”€ routes/             # API è·¯ç”±
â”‚   â””â”€â”€ gpu.py         # GPU ç®¡ç†æ¥å£
â”‚
â”œâ”€â”€ services/           # ä¸šåŠ¡é€»è¾‘
â”‚   â””â”€â”€ gpu_service.py # GPU ç›‘æ§æœåŠ¡
â”‚
â”œâ”€â”€ models/             # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ gpu.py         # GPU æ•°æ®ç»“æ„
â”‚
â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ common.py      # é€šç”¨å·¥å…·
â”‚
â””â”€â”€ tests/              # æµ‹è¯•æ–‡ä»¶
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### GPU çŠ¶æ€ç›‘æ§

API å®æ—¶ç›‘æ§ GPU çš„ï¼š
- æ˜¾å­˜ä½¿ç”¨æƒ…å†µ (æ€»é‡/å·²ç”¨/å¯ç”¨)
- GPU åˆ©ç”¨ç‡
- æ˜¾å­˜åˆ©ç”¨ç‡
- æ¸©åº¦ (GPU/æ˜¾å­˜)
- åŠŸè€— (å½“å‰/é™åˆ¶)
- å½“å‰ä»»åŠ¡çŠ¶æ€
- é©±åŠ¨ç‰ˆæœ¬

### å†…å­˜ä¼˜åŒ–

- åå°çº¿ç¨‹æŒç»­ç›‘æ§ (5 ç§’é—´éš”)
- 10 ç§’ç¼“å­˜æœºåˆ¶
- å¼‚æ­¥ä»»åŠ¡æ³¨å†Œ/æ³¨é”€
- ä»»åŠ¡è¿›åº¦è·Ÿè¸ª

## ğŸ“¡ API æ¥å£

### 1. è·å–æ‰€æœ‰ GPU çŠ¶æ€

```bash
curl http://localhost:8080/api/v1/gpu/status
```

**å“åº”:**
```json
{
  "code": 200,
  "data": {
    "gpus": [
      {
        "gpu_id": 0,
        "name": "NVIDIA GeForce RTX 4090",
        "memory": {
          "total": 24576,
          "used": 8192,
          "free": 16384,
          "utilization": 33
        },
        "utilization_gpu": 75,
        "temperature": {
          "gpu": 65
        },
        "status": "available"
      }
    ],
    "summary": {
      "total_gpus": 2,
      "available_gpus": 1,
      "busy_gpus": 1
    }
  }
}
```

### 2. è·å–å¯ç”¨ GPU

```bash
# æ‰€æœ‰å¯ç”¨ GPU
curl http://localhost:8080/api/v1/gpu/available

# è‡³å°‘ 10GB æ˜¾å­˜çš„å¯ç”¨ GPU
curl "http://localhost:8080/api/v1/gpu/available?min_memory=10000"
```

### 3. è·å– GPU è¯¦æƒ…

```bash
curl http://localhost:8080/api/v1/gpu/0/details
```

### 4. GPU æ±‡æ€»

```bash
curl http://localhost:8080/api/v1/gpu/summary
```

## ğŸ Python å®¢æˆ·ç«¯ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
import requests

API_BASE = "http://localhost:8080/api/v1"

# 1. è·å–æ‰€æœ‰ GPU
response = requests.get(f"{API_BASE}/gpu/status")
data = response.json()

print(f"å‘ç° {len(data['data']['gpus'])} ä¸ª GPU")

# 2. æŸ¥æ‰¾å¯ç”¨ GPU
available_gpus = [
    gpu for gpu in data['data']['gpus']
    if gpu['status'] == 'available'
]

if available_gpus:
    gpu = available_gpus[0]
    print(f"\nä½¿ç”¨ GPU {gpu['gpu_id']}: {gpu['name']}")
    print(f"å¯ç”¨æ˜¾å­˜: {gpu['memory']['free']} MB")
```

### é«˜çº§ç­›é€‰

```python
# ç­›é€‰å¯ç”¨æ˜¾å­˜ > 10GB çš„ GPU
response = requests.get(
    f"{API_BASE}/gpu/available",
    params={"min_memory": 10000}
)

gpus = response.json()['data']['available_gpus']

# é€‰æ‹©æœ€ç©ºé—²çš„ GPU
if gpus:
    best_gpu = min(gpus, key=lambda x: x['memory']['utilization'])
    print(f"é€‰æ‹© GPU {best_gpu['gpu_id']} (æ˜¾å­˜åˆ©ç”¨ç‡ {best_gpu['memory']['utilization']}%)")
```

### å®æ—¶ç›‘æ§

```python
import time

for i in range(10):
    response = requests.get(f"{API_BASE}/gpu/status")
    data = response.json()

    for gpu in data['data']['gpus']:
        print(f"GPU {gpu['gpu_id']}: åˆ©ç”¨ç‡ {gpu['utilization_gpu']}%")

    time.sleep(5)  # æ¯ 5 ç§’ç›‘æ§ä¸€æ¬¡
```

## ğŸ› ï¸ å¸¸ç”¨å‘½ä»¤

```bash
# å®‰è£…ä¾èµ–
make install

# å¯åŠ¨æœåŠ¡
make dev

# åå°è¿è¡Œ
make background

# è¿è¡Œæµ‹è¯•
make test

# æŸ¥çœ‹æ—¥å¿—
make logs

# åœæ­¢æœåŠ¡
make stop

# æŸ¥çœ‹çŠ¶æ€
make status

# æ¸…ç†æ—¥å¿—
make clean

# æŸ¥çœ‹å¸®åŠ©
make help
```

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

### ç¼“å­˜æœºåˆ¶
- GPU ä¿¡æ¯ç¼“å­˜ 10 ç§’
- å‡å°‘ NVML è°ƒç”¨å¼€é”€
- æå‡ API å“åº”é€Ÿåº¦

### å¼‚æ­¥ç›‘æ§
- åå°çº¿ç¨‹æŒç»­ç›‘æ§
- ä¸é˜»å¡ä¸» API è¯·æ±‚
- å®æ—¶æ›´æ–° GPU çŠ¶æ€

### å†…å­˜æ•ˆç‡
- æŒ‰éœ€è·å– GPU ä¿¡æ¯
- é¿å…é‡å¤æŸ¥è¯¢
- æ™ºèƒ½ç¼“å­˜æ›´æ–°

## ğŸ” æ•…éšœæ’æŸ¥

### 1. NVML åˆå§‹åŒ–å¤±è´¥

**ç—‡çŠ¶:**
```
Warning: NVML initialization failed
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ£€æŸ¥é©±åŠ¨
nvidia-smi

# æ£€æŸ¥ CUDA
nvcc --version

# API ä¼šè‡ªåŠ¨é™çº§åˆ°æ¨¡æ‹Ÿæ•°æ®
```

### 2. è¿æ¥è¢«æ‹’ç»

**ç—‡çŠ¶:**
```
requests.exceptions.ConnectionError
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
make status

# æŸ¥çœ‹æ—¥å¿—
make logs

# é‡æ–°å¯åŠ¨
make dev
```

### 3. GPU ä¿¡æ¯ä¸å‡†ç¡®

**åŸå› :**
- ç›‘æ§æœ‰å»¶è¿Ÿ
- ä»»åŠ¡çŠ¶æ€æ›´æ–°æ…¢

**è§£å†³æ–¹æ¡ˆ:**
```python
# ç­‰å¾…å‡ ç§’åé‡è¯•
time.sleep(10)

# å¼ºåˆ¶åˆ·æ–°
response = requests.get(f"{API_BASE}/gpu/status")
```

## ğŸ“ˆ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°æ¥å£

1. **åˆ›å»ºè·¯ç”±** (`routes/example.py`)

```python
from flask import Blueprint
from services.example_service import example_service

example_bp = Blueprint('example', url_prefix='/api/v1/example')

@example_bp.route('/test', methods=['GET'])
def test():
    data = example_service.get_data()
    return jsonify({"data": data})
```

2. **æ³¨å†Œè“å›¾** (`app.py`)

```python
from routes import example
app.register_blueprint(example.example_bp)
```

### æ‰©å±• GPU ç›‘æ§

```python
# services/gpu_service.py
def _get_gpu_additional_info(self, handle):
    """è·å–é¢å¤– GPU ä¿¡æ¯"""
    info = {}

    try:
        # é£æ‰‡è½¬é€Ÿ
        info['fan_speed'] = pynvml.nvmlDeviceGetFanSpeed(handle)

        # æ—¶é’Ÿé¢‘ç‡
        info['graphics_clock'] = pynvml.nvmlDeviceGetClockInfo(
            handle, pynvml.NVML_CLOCK_GRAPHICS
        )

        # æ˜¾å­˜é¢‘ç‡
        info['memory_clock'] = pynvml.nvmlDeviceGetClockInfo(
            handle, pynvml.NVML_CLOCK_MEM
        )
    except pynvml.NVMLError:
        pass

    return info
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
try:
    response = requests.get(f"{API_BASE}/gpu/status")
    response.raise_for_status()
    data = response.json()
except requests.exceptions.RequestException as e:
    print(f"API è¯·æ±‚å¤±è´¥: {e}")
    # ä½¿ç”¨ç¼“å­˜æ•°æ®æˆ–é»˜è®¤å€¼
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# ç¼“å­˜ GPU åˆ—è¡¨ (30 ç§’)
GPU_CACHE_TTL = 30

cached_gpus = None
last_update = 0

def get_cached_gpus():
    global cached_gpus, last_update
    now = time.time()

    if now - last_update > GPU_CACHE_TTL:
        response = requests.get(f"{API_BASE}/gpu/status")
        cached_gpus = response.json()
        last_update = now

    return cached_gpus
```

### 3. ç›‘æ§å‘Šè­¦

```python
def check_gpu_health():
    response = requests.get(f"{API_BASE}/gpu/status")
    data = response.json()

    for gpu in data['data']['gpus']:
        # æ¸©åº¦å‘Šè­¦
        if gpu['temperature']['gpu'] > 80:
            print(f"è­¦å‘Š: GPU {gpu['gpu_id']} æ¸©åº¦è¿‡é«˜ ({gpu['temperature']['gpu']}Â°C)")

        # æ˜¾å­˜ä¸è¶³å‘Šè­¦
        if gpu['memory']['free'] < 1000:  # å°äº 1GB
            print(f"è­¦å‘Š: GPU {gpu['gpu_id']} æ˜¾å­˜ä¸è¶³")
```

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´ API æ–‡æ¡£**: `/root/diffusion-pipe/TRAINING_API.md`
- **README**: `/root/diffusion-pipe/api/README.md`
- **é¡¹ç›®æ¶æ„**: `/root/diffusion-pipe/ARCHITECTURE.md`

## ğŸ†˜ è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹å¸®åŠ©
make help

# æŸ¥çœ‹æ—¥å¿—
make logs

# è¿è¡Œæµ‹è¯•
make test
```

---

**å¿«é€Ÿé“¾æ¥:**
- [API æ–‡æ¡£](./README.md)
- [å®Œæ•´è®­ç»ƒ API è®¾è®¡](../TRAINING_API.md)
- [é¡¹ç›®æ¶æ„](../ARCHITECTURE.md)
